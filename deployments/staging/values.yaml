# Staging Environment - Moderate Resources
# HA configuration, production-like setup

# TiDB Operator
# The operator manages TiDB clusters. The actual TiDB cluster is created separately.
tidb:
  enabled: true
  chart:
    repo: https://charts.pingcap.org/
    name: tidb-operator
    version: "~1.6.0"
  values:
    # Disable scheduler - it has config compatibility issues
    scheduler:
      create: false
    # Minimal resources for staging
    controllerManager:
      replicas: 1
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 500m
          memory: 512Mi

# TiDB Cluster - Staging configuration with reduced resources
tidbCluster:
  enabled: true
  pd:
    replicas: 3
    storage: "10Gi"
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 500m
        memory: 1Gi
  tikv:
    replicas: 3
    storage: "50Gi"
    resources:
      requests:
        cpu: 500m
        memory: 2Gi
      limits:
        cpu: 1000m
        memory: 4Gi
  tidb:
    replicas: 2
    resources:
      requests:
        cpu: 200m
        memory: 512Mi
      limits:
        cpu: 1000m
        memory: 2Gi

# Valkey
valkey:
  enabled: true
  chart:
    repo: https://valkey.io/valkey-helm/
    name: valkey
    version: "0.7.0"
  values:
    architecture: replication
    auth:
      enabled: true
      password: "changeme123"
    master:
      persistence:
        size: 10Gi
    replica:
      replicaCount: 2
      persistence:
        size: 10Gi

# MinIO
minio:
  enabled: true
  chart:
    repo: https://charts.min.io/
    name: minio
    version: "5.x.x"
  values:
    mode: distributed
    replicas: 4
    drivesPerNode: 1
    persistence:
      size: 50Gi
    rootUser: admin
    rootPassword: "changeme123"
    buckets:
      - name: pm-tp-files
        policy: none

# NATS
nats:
  enabled: true
  chart:
    repo: https://nats-io.github.io/k8s/helm/charts/
    name: nats
    version: "1.x.x"
  values:
    nats:
      jetstream:
        enabled: true
        fileStore:
          pvc:
            size: 20Gi
    cluster:
      enabled: true
      replicas: 3

# OpenSearch - Search Engine
# Temporarily disabled - requires vm.max_map_count=262144 on nodes
# opensearch:
#   enabled: false
#   chart:
#     repo: https://opensearch-project.github.io/helm-charts/
#     name: opensearch
#     version: "2.x.x"
#   values:
#     replicas: 3
#     minimumMasterNodes: 2
#     persistence:
#       size: 30Gi
#     resources:
#       requests:
#         cpu: 1000m
#         memory: 2Gi

# Keycloak - Identity and Access Management (removed from Fleet - deployed manually)
# keycloak:
#   enabled: false

# Prometheus
prometheus:
  enabled: true
  chart:
    repo: https://prometheus-community.github.io/helm-charts
    name: prometheus
    version: "25.x.x"
  values:
    server:
      retention: 15d
      persistentVolume:
        size: 30Gi
    alertmanager:
      enabled: true

# Grafana
grafana:
  enabled: true
  chart:
    repo: https://grafana.github.io/helm-charts
    name: grafana
    version: "7.x.x"
  values:
    adminUser: admin
    adminPassword: "changeme123"
    persistence:
      enabled: true
      size: 10Gi
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            url: http://prometheus-server
            isDefault: true

# Harbor - Container Registry
# Disabled for now - RWO volume conflicts with RollingUpdate strategy
# Will revisit in a dedicated Harbor deployment project
harbor:
  enabled: false
  values:
    global:
      hosts:
        core: harbor.rancher-poc.1.todevopssandbox.com
        portal: harbor.rancher-poc.1.todevopssandbox.com
        registry: harbor.rancher-poc.1.todevopssandbox.com
    expose:
      type: ingress
      tls:
        enabled: true
        certSource: secret
        secret:
          secretName: harbor-tls-secret
      ingress:
        enabled: false
        hosts:
          core: harbor.rancher-poc.1.todevopssandbox.com
        controller: default
    externalURL: https://harbor.rancher-poc.1.todevopssandbox.com
    hostname: harbor.rancher-poc.1.todevopssandbox.com
    harborAdminPassword: "Harbor12345"
    persistence:
      enabled: true
      resourcePolicy: "keep"
      persistentVolumeClaim:
        registry:
          size: 10Gi
        chartmuseum:
          size: 5Gi
        jobservice:
          size: 1Gi
        database:
          size: 1Gi
        redis:
          size: 1Gi
        trivy:
          size: 5Gi
    core:
      replicas: 1
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
    jobservice:
      replicas: 1
      # Use Recreate strategy due to RWO volume constraints
      deploymentStrategy:
        type: Recreate
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 300m
          memory: 256Mi
    registry:
      replicas: 1
      # Use Recreate strategy due to RWO volume constraints
      deploymentStrategy:
        type: Recreate
      resources:
        requests:
          cpu: 200m
          memory: 256Mi
        limits:
          cpu: 500m
          memory: 512Mi
    portal:
      replicas: 1
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 300m
          memory: 256Mi
    database:
      type: internal
      resources:
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 300m
          memory: 512Mi
    redis:
      type: internal
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 300m
          memory: 256Mi
    trivy:
      enabled: false  # Disabled for staging to save resources
    notary:
      enabled: false

# Traefik - Ingress Controller
traefik:
  enabled: true
  values:
    deployment:
      replicas: 1
      annotations: {}
      podAnnotations: {}
      additionalContainers: []
      initContainers: []
    ports:
      traefik:
        port: 9000
        expose:
          default: true
        exposedPort: 9000
        protocol: TCP
      web:
        port: 8000
        expose:
          default: true
        exposedPort: 80
        protocol: TCP
        redirectTo:
          port: websecure
      websecure:
        port: 8443
        expose:
          default: true
        exposedPort: 443
        protocol: TCP
        tls:
          enabled: true
    ingressRoute:
      dashboard:
        enabled: false
    providers:
      kubernetesCRD:
        enabled: true
        allowCrossNamespace: true
        allowExternalNameServices: true
      kubernetesIngress:
        enabled: true
        allowExternalNameServices: true
        publishedService:
          enabled: false
    rbac:
      enabled: true
    serviceAccount:
      name: ""
    service:
      enabled: true
      type: LoadBalancer
      annotations: {}
      labels: {}
      spec:
        loadBalancerIP: ""
      externalIPs: []
      loadBalancerSourceRanges: []
    persistence:
      enabled: false
    logs:
      general:
        level: INFO
      access:
        enabled: false
    tracing:
      instana: {}
      jaeger: {}
      zipkin: {}
      datadog: {}
      haystack: {}
    metrics:
      prometheus: {}
      datadog: {}
      influxdb: {}
      statsd: {}
    pilot:
      enabled: false
    experimental:
      plugins:
        enabled: false
    additionalArguments:
      - "--certificatesresolvers.le.acme.email=admin@yourdomain.com"
      - "--certificatesresolvers.le.acme.storage=/data/acme.json"
      - "--certificatesresolvers.le.acme.tlschallenge=true"
      - "--certificatesresolvers.le.acme.httpchallenge=true"
      - "--certificatesresolvers.le.acme.httpchallenge.entrypoint=web"
    env: []
    envFrom: []
    hostNetwork: false
    securityContext:
      capabilities:
        drop: [ALL]
        add: [NET_BIND_SERVICE]
      readOnlyRootFilesystem: true
      runAsGroup: 65532
      runAsNonRoot: true
      runAsUser: 65532
    podSecurityContext:
      fsGroup: 65532
    terminationGracePeriodSeconds: 60
    affinity: {}
    tolerations: []
    nodeSelector: {}
    topologySpreadConstraints: []
    dnsPolicy: ClusterFirst
    dnsConfig: {}
    priorityClassName: ""
    podDisruptionBudget:
      enabled: false
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 300m
        memory: 512Mi
